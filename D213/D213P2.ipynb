{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D213 Advanced Data Analytics PA 1\n",
    "##### Submitted By Edwin Perry\n",
    "### Table of Contents\n",
    "<ol>\n",
    "    <li><a href=\"#A\">Research Question</a></li>\n",
    "    <li><a href=\"#B\">Data Preparation</a></li>\n",
    "    <li><a href=\"#C\">Network Architecture</a></li>\n",
    "    <li><a href=\"#D\">Model Evaluation</a></li>\n",
    "    <li><a href=\"#E\">Summary and Recommendations</a></li>\n",
    "    <li><a href=\"#F\">Reporting</a></li>\n",
    "</ol>\n",
    "<h4 id=\"A\">Research Question</h4>\n",
    "<h5>Providing Question</h5>\n",
    "<p>For this project, I want to determine if a neural network trained on customer reviews is adequate at predicting overall customer sentiment</p>\n",
    "<h5>Objectives/Goal</h5>\n",
    "<p>The overall goal of this process is to create a neural network capable of accurately predicting the customer rating of transactions and services based on the textual review the customer has left\n",
    "\n",
    "The use of a neural network to recognize positive or negative user sentiment about a movie based on textual review is a relatively complex natural language processing (NLP) technique. The used dataset will include a binary decision as to whether or not the customer enjoyed the movie, which can be used as a \"truth\" or \"false\" value, allowing me to test the model predictions against the actual values.\n",
    "\n",
    "The neural network created from this process could be used for a number of future uses. Perhaps the most useful that I can think of is fake review detection. Multiple products and services are known to suffer from fake reviews submitted by bots, rather than reviews from people that have actually watched the movie. \n",
    "This neural network can identify those reviews that do not align the textual review with the provided rating, helping to root out the most likely fake reviews.</p>\n",
    "<h5>Type of Neural Network</h5>\n",
    "<p>Neural networks come in a variety of types, and as such, it is important to identify the ideal type to use for this particular analysis. The type that I decided to use for this analysis is a Recurrent Neural Network (RNN). I decided to use this type because, rather than analyzing the data in the text review in a manner whereby it is simply checking the value of words, it instead takes into account the word order as well. For example, if a user were to say \"I do not recommend this movie,\" most other models would consider each word independently from each other, and would likely conclude that the rating would be positive. The RNN, however, can take into account the combination of the words \"not\" and \"recommend\" being sequential, recognizing that the user has a negative review.</p>\n",
    "<h4 id=\"B\">Data Preparation</h4>\n",
    "<p> There are a few tasks that neeed to be performed before the neural network can be created and tested, including performing some exploratory data analysis. The first thing that we will do is import the required libraries/packages and load the data into the Jupyter Notebook</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edwinp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n",
      "amazon_cells_labelled.txt: 1000 lines\n",
      "imdb_labelled.txt: 1000 lines\n",
      "yelp_labelled.txt: 1000 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Initialize a dictionary to store the data\n",
    "data_dict = {}\n",
    "# Define the folder path\n",
    "folder_path = './sentiment+labelled+sentences'\n",
    "full_folder_path = './sentiment+labelled+sentences/sentiment labelled sentences'\n",
    "\n",
    "# List all files in the folder\n",
    "files = [f for f in os.listdir(full_folder_path) if f != 'readme.txt']\n",
    "\n",
    "# Initialize a dictionary to store file lengths\n",
    "file_lengths = {}\n",
    "print(files)\n",
    "# Read each file and calculate its length\n",
    "for file in files:\n",
    "    with open(os.path.join(full_folder_path, file), 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            text = line[0:-2]\n",
    "            label = line[-2]\n",
    "            data_dict[text.strip()] = int(label)\n",
    "            file_lengths[file] = len(content)\n",
    "        \n",
    "\n",
    "# Print the lengths of each file\n",
    "for file, length in file_lengths.items():\n",
    "    print(f\"{file}: {length} lines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Above, we can notice that we have 1000 lines in each file, giving us access to 3000 rows of data total. Next, we will look into the presence of unusual characters within the data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys with unusual characters:\n",
      "It's practically perfect in all of them  a true masterpiece in a sea of faux \"masterpieces.\n",
      "I'm glad this pretentious piece of s*** didn't do as planned by the Dodge stratus Big Shots... It's gonna help movie makers who aren't in the very restrained \"movie business\" of Québec.\n",
      "The script iswas there a script?\n",
      "I'll even say it again  this is torture.\n",
      "This show is made for Americans - it is too stupid and full with hatred and clichés to be admitted elsewhere.\n",
      "A cheap and cheerless heist movie with poor characterisation, lots of underbite style stoic emoting (think Chow Yun Fat in A Better Tomorrow) and some cheesy clichés thrown into an abandoned factory ready for a few poorly executed flying judo rolls a la John Woo.\n",
      "And I forgot: The Casting here i superb, with Trond Fausa Aurvåg being perfect in the role as the Bothersome Man, who doesn't understand where he is, what he is doing and why.\n",
      "The script is bad, very bad  it contains both cheesiness and unethical joke that you normally see in rated R or NC-17 movie.\n",
      "Let's start with all the problemsthe acting, especially from the lead professor, was very, very bad.\n",
      "Technically, the film is well made with impressive camera-work, solid acting and effective music from Riz Ortolani  particularly good is a recurring unaccompanied female vocal that sounds like it's coming from a distant hill.\n",
      "I am so tired of clichés that is just lazy writing, and here they come in thick and fast.\n",
      "But, Kevin Spacey is an excellent, verbal tsunami as Buddy Ackerman  and totally believable because he is a great actor.\n",
      "Definitely worth seeing it's the sort of thought provoking film that forces you to question your own threshold of loneliness.\n",
      "My fiancé and I came in the middle of the day and we were greeted and seated right away.\n",
      "I really enjoyed Crema Café before they expanded; I even told friends they had the BEST breakfast.\n",
      "The crêpe was delicate and thin and moist.\n",
      "The only thing I wasn't too crazy about was their guacamole as I don't like it puréed.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to check for non-English characters\n",
    "def contains_unusual_characters(text):\n",
    "    # Define a regex pattern to match non-English characters and emojis\n",
    "    pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "# Initialize a list to store keys with unusual characters\n",
    "unusual_keys = []\n",
    "\n",
    "# Iterate through the keys in data_dict and check for unusual characters\n",
    "for key in data_dict.keys():\n",
    "    if contains_unusual_characters(key):\n",
    "        unusual_keys.append(key)\n",
    "\n",
    "# Print the keys with unusual characters\n",
    "print(\"Keys with unusual characters:\")\n",
    "for key in unusual_keys:\n",
    "    print(key)\n",
    "print(len(unusual_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Obviously, we can see that only 17 rows contain these unusual characters. Due to the incredibly small number of unusual keys, I believe that filtering them from our analysis is justified.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data_dict length: 2982\n",
      "Filtered data_dict length: 2965\n"
     ]
    }
   ],
   "source": [
    "# Filter out keys that are present in unusual_keys\n",
    "filtered_data_dict = {key: value for key, value in data_dict.items() if key not in unusual_keys}\n",
    "\n",
    "# Print the length of the filtered dictionary to verify\n",
    "print(f\"Original data_dict length: {len(data_dict)}\")\n",
    "print(f\"Filtered data_dict length: {len(filtered_data_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these filtered values are removed, we will proceed to determine the vocabulary size of the reviewers in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/edwinp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/edwinp/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "# Ensure 'punkt' is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Tokenize the text in the keys of filtered_data_dict\n",
    "all_words = []\n",
    "for review in filtered_data_dict.keys():\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "# Count the unique words\n",
    "vocabulary_size = len(set(all_words))\n",
    "\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will propose a word embedding length. This will be determined using the logarithm of the vocabulary size, to ensure that the dataset is not filtered such that it is too small to notice critical information, but that we are filtering out enough data to avoid unnecessary computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed embedding length: 13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the proposed embedding length\n",
    "proposed_embedding_length = math.ceil(math.log2(vocabulary_size))\n",
    "print(f\"Proposed embedding length: {proposed_embedding_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will determine the maximum sequence length for this analysis. This will be done by determining the 95th percentile of review lengths, and using that value as the maximum sequence length. This will allow the reviews to have a length-constraint that optimizes model performance by filtering out excessively long reviews while also retaining the vast majority of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistically justified maximum sequence length: 30\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lengths of the reviews\n",
    "review_lengths = [len(nltk.word_tokenize(review)) for review in filtered_data_dict.keys()]\n",
    "\n",
    "# Calculate the 95th percentile of review lengths\n",
    "max_sequence_length = np.percentile(review_lengths, 95)\n",
    "max_sequence_length = int(max_sequence_length)\n",
    "print(f\"Statistically justified maximum sequence length: {max_sequence_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Goals of Tokenization Process</h5>\n",
    "<p>The next step in the data preparation is tokenizing the text reviews. Neural networks cannot inherently interpret raw text, so we need to break the text into smaller units that map to numeric values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (2965,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(filtered_data_dict.keys())\n",
    "\n",
    "# Convert the text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(filtered_data_dict.keys())\n",
    "\n",
    "# Convert the labels to a numpy array\n",
    "labels = np.array(list(filtered_data_dict.values()))\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Padding Process Explanation</h5>\n",
    "<p>Padding is an essential process in the preparation of this data for the neural network. It is designed to standardize the size of the inputs, which inherently have different lengths. I am going to add the padding to the end of the sequence, using the padding='post' argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences shape: (2965, 30)\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to ensure they all have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Below, we see one example of a padded sequence. The several entries of 0 at the end indicate padded values</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  28   47    6   57  117   13   71    8  370    7   12   66   12    2\n",
      "  185  578    4   76   61    5 2243    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Print a single padded sequence\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Sentiment Categories</h5>\n",
    "<p>There will only be 2 categories of sentiment used in this analysis, as we only have positive and negative reviews for this data, meaning that we have binary classification. The activation function will similarly be sigmoid, which will allow us to determine the positive or negative value based on whether the calculated value meets a certain threshold</p>\n",
    "<h5>Steps Explanation</h5>\n",
    "<p>To review the steps of the data preparation, we started, after importing the data, with filtering out any reviews that contained characters that couldn't be interpreted. Then, we determined the vocabulary size, proposed a word embedding length, and calculated a statistically-justified maximum sequence length. Then, we tokenized the reviews to convert the textual, unusable data into numerical data that could be used by the neural network. The next step was to pad the data, to ensure that the inputs were of a standard length. Now, we will split the data into training, validation, and testing sets. An 80%/10%/10% split is most common in a neural network. The training set, of course, requires the majority of the data, to train it in the widest variety of information possible and ensure the best performance. A 10% validation set is technically optional, but it is ideal. The validation set is useful in detecting overfitting and can fine-tine hyperparameters, improving the performance of the model. Finally, we use a 10% test set, to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2075, 30)\n",
      "Validation data shape: (445, 30)\n",
      "Testing data shape: (445, 30)\n",
      "Training labels shape: (2075,)\n",
      "Validation labels shape: (445,)\n",
      "Testing labels shape: (445,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and temporary datasets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary dataset into validation and test datasets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to pandas DataFrames\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['label'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['label'])\n",
    "y_validation_df = pd.DataFrame(y_train, columns=['label'])\n",
    "X_validation_df = pd.DataFrame(y_test, columns=['label'])\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "X_train_df.to_csv('X_train.csv', index=False)\n",
    "X_test_df.to_csv('X_test.csv', index=False)\n",
    "y_train_df.to_csv('y_train.csv', index=False)\n",
    "y_test_df.to_csv('y_test.csv', index=False)\n",
    "X_validation_df.to_csv('X_validation.csv', index=False)\n",
    "y_validation_df.to_csv('y_validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
